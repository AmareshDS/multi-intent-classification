{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-02 : Multi-label text classification\n",
    "\n",
    "After extracting intents, we use Keras, a comprehensive deep learning library, to develop a multi-class classification model.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Large-scale multi-label text classification](https://keras.io/examples/nlp/multi_label_classification/)\n",
    "- [A Smooth F1 Score Surrogate Loss for Multilabel Classification](https://arxiv.org/abs/2108.10566)\n",
    "- [Bidirectional LSTM on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)\n",
    "- [Perform text classification with KerasNLP](https://www.youtube.com/watch?v=Wb8-1O8bW68)\n",
    "- [Getting Started with KerasNLP](https://keras.io/guides/keras_nlp/getting_started/)\n",
    "- [Getting Started with KerasNLP (Notebook)](https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_nlp/getting_started.ipynb)\n",
    "- [TF2: Pre-Train BERT from scratch (a Transformer), fine-tune & run inference on text | KERAS NLP](https://www.youtube.com/watch?v=W735DaBOKKo)\n",
    "- [2023 KerasNLP Tutorial: Explore Latest KERAS Toolbox & NLP Processing Library for BERT - TF](https://www.youtube.com/watch?v=XL9vDoumP7Y)\n",
    "- [BertClassifier model](https://keras.io/api/keras_nlp/models/bert/bert_classifier/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "orig_data_path = f'{data_path}/hellopeter'\n",
    "orig_file = f'{orig_data_path}/00-01_vodacom_selected_reviews.parquet.gz'\n",
    "\n",
    "intent_path = f'{data_path}/multiclass_model'\n",
    "intent_extract_file = f'{intent_path}/01-03_intents.parquet.gz'\n",
    "intent_file = f'{intent_path}/02-01_flat_intents.parquet.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data Lineage\n",
    "\n",
    "1. The original dataset is a collection publicly accessible customer reviews/complaints scraped from [Hellopeter](https://www.hellopeter.com/) site between 2021 and 2023. This dataset was [created](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/01_hellopeter/01-01_retrieve_data.ipynb) in another research project investigating: [Evaluating Customer Satisfaction and Preferences in the Telecommunications Industry: A Comparative Analysis of Survey Data and Online Reviews](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/04_draft/04-04_cw02.ipynb)\n",
    "\n",
    "2. Data selection was perform in the `00-01_data_selection` notebook based on the following criteria.\n",
    "    - Reviews from the for the 5 month period from **2022-06-01** to **2023-06-30** were selected.\n",
    "\n",
    "    - Only reviews from the **Vodacom** telecommunications company were selected.\n",
    "\n",
    "    - Very short, or very long reviews were removed. Reviews between from **10** to **100** words were selected. The word count was calculated using a basic `.str.split().str.len()` which is sufficient for this purpose.\n",
    "\n",
    "3. The unlabeled data was then labeled in the `01-02_batch_classification` notebook using Generative AI.\n",
    "    - The **Mistral 7B v0.2** Large Language Model (LLM) were hosted on the local servier with [Ollama](https://ollama.com/library/mistral). Please refer to the `01-01_classification_test` notebook for further details.\n",
    "\n",
    "    - The classification was done using multiple prompts similar to Chain-of-Thought (CoT) techniques for classification. _Implementation details can be found in the `src` directory._\n",
    "\n",
    "    - Classification was done based on the categories defined in `src/config/category_definitions.jsonl`.\n",
    "\n",
    "    - It took an average of **7 seconds** to classify a single review.\n",
    "\n",
    "4. Using Generative AI for labeling introduced new categories that were cleaned up in the `01-03_cleanup` notebook.\n",
    "\n",
    "    - First new categories that were prefixed with an original category were replaced.\n",
    "\n",
    "    - Then, new categories that contained an original category in round brackets were replaced with the original.\n",
    "\n",
    "    - For the remaining new categories, the reviews were manually inspected and the reviews were reclassified via manual mapping.\n",
    "\n",
    "5. The data labels was then converted into a format suitable for modeling in the `02-01_data_preperation` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Source Data\n",
    "\n",
    "The following shows a sample customer review from the original source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = pd.read_parquet(orig_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_source[df_source.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Data Labels\n",
    "\n",
    "A sample of the labels extracted using the LLM is shown below. From this we can see that the LLM has extracted multiple labels for each review, and a reason is generated form each label for human verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intents_extracted = pd.read_parquet(intent_extract_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents_extracted[df_intents_extracted.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 .Prepared Data Labels\n",
    "\n",
    "The prepared data labels are shown below. The data labels are prepared for multi-label classification.\n",
    "\n",
    "This data will need to be combined with the `review_title` and `review_content` from the original source data to create the final dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intents = pd.read_parquet(intent_file)\n",
    "df_intents[\"category_list\"] = df_intents[\"category_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"relevance_list\"] = df_intents[\"relevance_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"sentiment_list\"] = df_intents[\"sentiment_list\"].apply(lambda x: list(x))\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents[df_intents.id == 3950575]\n",
    "    )\n",
    "\n",
    "print(f'Data samples: {len(df_source)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of category target values\n",
    "categories = tf.ragged.constant(df_intents.category_list.values)\n",
    "\n",
    "# learn the vocabulary\n",
    "lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(categories)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(lookup.get_vocabulary())\n",
    "print(f'Vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sample illustrates the binarization of the multi-labels. The multiple labels are encoded into a binary matrix where the positions corresponding with the labels have a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a sample label to test the lookup\n",
    "sample_label = df_intents[df_intents.id == 3950575].category_list.values[0]\n",
    "\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "# binarize the label\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `invert_multi_hot` function is used to convert the binary matrix back into the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels, lookup:keras.layers.StringLookup):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    # get the vocabulary\n",
    "    vocab = lookup.get_vocabulary()\n",
    "\n",
    "    hot_indices = np.argwhere(encoded_labels == 1)[..., 0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "## test the inverse function\n",
    "invert_multi_hot(label_binarized[0], lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare Dataset\n",
    "\n",
    "Prepare a dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the original label dataset\n",
    "df_preprocess = df_intents.copy()\n",
    "\n",
    "# binarize the category_list\n",
    "labels = tf.ragged.constant(df_preprocess[\"category_list\"].values)\n",
    "label_binarized = lookup(labels).numpy()\n",
    "df_preprocess[\"category_encoded\"] = label_binarized.tolist()\n",
    "\n",
    "# add the review content\n",
    "df_preprocess = df_preprocess \\\n",
    "    .set_index('id') \\\n",
    "    .join(df_source.set_index('id'), how='left') \\\n",
    "    .assign(review_text=lambda x: '**' + x.review_title + '**\\n\\n' + x.review_content) \\\n",
    "    .reset_index()\n",
    "\n",
    "# select the relevant columns\n",
    "df_preprocess = df_preprocess[['id', 'review_text', 'category_encoded']]\n",
    "\n",
    "# show a sample of the preprocessed data\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_preprocess.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Find the Most Frequent Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique label combinations\n",
    "df_combinations = df_preprocess \\\n",
    "    .category_encoded \\\n",
    "    .value_counts(normalize=True) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'category_encoded': 'combination'})\n",
    "\n",
    "# show the value and percentage of the top combination\n",
    "top_row = df_combinations.iloc[0]\n",
    "top_combination = np.array(top_row['combination']).astype(int)\n",
    "top_proportion = top_row['proportion']\n",
    "\n",
    "print(f'Top Combination: {top_combination}')\n",
    "print(f'Proportion: {top_proportion:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Train Test Split\n",
    "\n",
    "In a multi-label classification problem, imbalance can occur at two levels:\n",
    "\n",
    "1. **Label imbalance**: Some labels appear more frequently than others.\n",
    "2. **Label combination imbalance**: Some combinations of labels appear more frequently than others.\n",
    "\n",
    "Both these imbalances are present in the dataset. Imbalance can lead to a model that performs well on the majority classes but poorly on the minority classes. This is because the model might be biased towards predicting the majority classes due to their higher occurrence in the training data.\n",
    "\n",
    "To address this, we will ideally use a stratified split to ensure that the distribution of labels in the training and validation sets is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_count = df_intents.category_list.value_counts().reset_index()\n",
    "df_category_count.columns = ['category', 'samples']\n",
    "\n",
    "print(f'Combination Category Count  : {len(df_category_count)}')\n",
    "print(f'Combinations with one sample: {len(df_category_count[df_category_count.samples == 1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we can see that about a third (0.34) of the unique category combinations have only one sample. This means that we will not be able to use a stratified split, as the validation set will not contain any of these unique combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# initial train and test split\n",
    "train_full_df, test_df = train_test_split(\n",
    "    df_intents,\n",
    "    test_size=test_split\n",
    ")\n",
    "\n",
    "# splitting the train set further into validation and new train sets\n",
    "val_df = train_full_df.sample(frac=0.2)\n",
    "train_df = train_full_df.drop(val_df.index)\n",
    "\n",
    "# select only the id column\n",
    "train_full_ids = train_full_df.id.values\n",
    "train_ids = train_df.id.values\n",
    "val_ids = val_df.id.values\n",
    "test_ids = test_df.id.values\n",
    "\n",
    "# show the record counts per dataset\n",
    "print(f\"Number of rows in training set   : {len(train_ids):>5}\")\n",
    "print(f\"Number of rows in validation set : {len(val_ids):>5}\")\n",
    "print(f\"Number of rows in test set       : {len(test_ids):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Make Datasets\n",
    "\n",
    "Use the test-train split to create the final `tf.data.Dataset` objects for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(ids:np.ndarray, df:pd.DataFrame, batch_size:int=32, shuffle:bool=False):\n",
    "    \"\"\"Create a tf.data.Dataset from a pandas DataFrame.\"\"\"\n",
    "    # filter the dataframe to only the selected ids\n",
    "    df = df[df.id.isin(ids)]\n",
    "\n",
    "    # get the encoded labels and review text\n",
    "    labels = np.array([np.array(lst) for lst in df.category_encoded.values])\n",
    "    review_text = df.review_text.values\n",
    "\n",
    "    # create the dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((review_text, labels))\n",
    "\n",
    "    # shuffle the dataset if needed\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size * 10)\n",
    "\n",
    "    # batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 \n",
    "\n",
    "train_dataset = make_dataset(train_ids, df_preprocess, batch_size, shuffle=True)\n",
    "val_dataset = make_dataset(val_ids, df_preprocess, batch_size, shuffle=False)\n",
    "test_dataset = make_dataset(test_ids, df_preprocess, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:3]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Review: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0], lookup=lookup)}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Based on the metrics described in the paper titled \"CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines\" (DOI: 10.48550/arXiv.2204.13746), we have selected the following metrics to estimate the performance of models on our multi-label classification dataset:\n",
    "\n",
    "1. **F1-Score (Macro-average and Weighted-average)**: \n",
    "    - **Justification**: The F1-Score is a harmonic mean of precision and recall, making it a balanced measure that considers both false positives and false negatives. The macro-average F1-Score treats all classes equally, emphasizing the importance of performing well across all classes, including the less frequent ones. The weighted-average F1-Score accounts for class imbalance by weighting the F1-Score of each class by its support (the number of true instances for each label). This is crucial in multi-label classification tasks where some labels may be more prevalent than others.\n",
    "    - **Calculation & Meaning**: The macro-average F1-Score is calculated by taking the average of the F1-Scores of all classes, while the weighted-average is computed by weighting each class's F1-Score by the number of true instances for that class. These metrics evaluate the model's overall performance across all labels, with macro-averaging treating every class equally and weighted-averaging considering the class distribution.\n",
    "\n",
    "2. **Jaccard Similarity**:\n",
    "    - **Justification**: The Jaccard Similarity, or Intersection over Union, is particularly well-suited for multi-label classification. It measures the similarity between the set of predicted labels and the set of true labels by dividing the size of the intersection by the size of the union of the predicted and true label sets. This metric is useful for evaluating performance in multi-label settings where the exact match might be too strict.\n",
    "    - **Calculation & Meaning**: The Jaccard Similarity is calculated for each instance and then averaged. A higher Jaccard score indicates a greater degree of overlap between the predicted and true labels, signifying better model performance in capturing the multilabel nature of the data.\n",
    "\n",
    "3. **Subset Accuracy (Exact Match)**:\n",
    "    - **Justification**: Subset accuracy is the strictest metric, requiring the set of predicted labels to exactly match the set of true labels for an instance to be considered correct. This metric is important in scenarios where the goal is to precisely predict the full set of applicable labels.\n",
    "    - **Calculation & Meaning**: Subset accuracy is the proportion of instances for which the predicted label set exactly matches the true label set. It provides a clear indication of the model's ability to perfectly predict label sets, though it may be too stringent for some multi-label applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score, accuracy_score\n",
    "\n",
    "class Evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_macro(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Macro-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_weighted(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Weighted-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(y_true, y_pred):\n",
    "        \"\"\"Calculate average Jaccard Similarity.\"\"\"\n",
    "        return jaccard_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    @staticmethod\n",
    "    def subset_accuracy(y_true, y_pred):\n",
    "        \"\"\"Calculate Subset Accuracy (Exact Match Accuracy).\"\"\"\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_all(y_true, y_pred):\n",
    "        \"\"\"Evaluate all metrics and display a summary.\"\"\"\n",
    "        f1_macro = Evaluation.f1_score_macro(y_true, y_pred)\n",
    "        f1_weighted = Evaluation.f1_score_weighted(y_true, y_pred)\n",
    "        jaccard_similarity = Evaluation.jaccard_similarity(y_true, y_pred)\n",
    "        subset_accuracy = Evaluation.subset_accuracy(y_true, y_pred)\n",
    "\n",
    "        # Display a summary of the evaluation\n",
    "        print(f\"F1 Score (Macro-Average)   \\t{f1_macro:.3f}\")\n",
    "        print(f\"F1 Score (Weighted-Average)\\t{f1_weighted:.3f}\")\n",
    "        print(f\"Average Jaccard Similarity \\t{jaccard_similarity:.3f}\")\n",
    "        print(f\"Subset Accuracy            \\t{subset_accuracy:.3f}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# Evaluation.evaluate_all(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model:keras.Model,\n",
    "                   dataset:tf.data.Dataset,\n",
    "                   threshold:float=0.5,\n",
    "                   verbose:int=1):\n",
    "    \"\"\"Evaluate the model using the Evaluation class.\"\"\"\n",
    "    # get the true labels\n",
    "    y_true = np.concatenate([label_batch.numpy() for _, label_batch in dataset], axis=0)\n",
    "\n",
    "    # get the predicted labels\n",
    "    y_pred = model.predict(dataset, verbose=verbose)\n",
    "    y_pred = y_pred > threshold\n",
    "\n",
    "    # evaluate the model\n",
    "    if verbose:\n",
    "        print()\n",
    "\n",
    "    Evaluation.evaluate_all(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_evaluation(model:keras.Model, verbose:int=1):\n",
    "    \"\"\"Show the validation results for the model.\"\"\"\n",
    "    global train_dataset\n",
    "    global val_dataset\n",
    "    global test_dataset \n",
    "\n",
    "    print(\"--- Training ------------------------\")\n",
    "    evaluate_model(model, train_dataset, verbose=verbose)\n",
    "    print()\n",
    "\n",
    "    print(\"--- Validation ----------------------\")\n",
    "    evaluate_model(model, val_dataset, verbose=verbose)\n",
    "    print()\n",
    "\n",
    "    print(\"--- Test ----------------------------\")\n",
    "    evaluate_model(model, test_dataset, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Vectorization\n",
    "\n",
    "Before we can train a model, we need to convert the text data into a format that can be fed into the model. There are multiple ways to do this, but a common approach is to convert text data into numerical data using a method such as TF-IDF, word embeddings, or a pre-trained language model.\n",
    "\n",
    "For this purpose we will use the a `TextVectorization` layer that can operate as part of the main model so that the model is excluded from the core preprocessing logic. As noted in the [Keras documentation](https://keras.io/examples/nlp/multi_label_classification/) this also greatly reduces the chances of training / serving skew during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of unique words in the dataset and the maximum length of a review.  \n",
    "_We expect the maximum review length to be 100 words as we have filtered the reviews to be between 10 and 100 words._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of unique words in the dataset\n",
    "vocabulary = set()\n",
    "df_preprocess.review_text.str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(f'Vocabulary size: {vocabulary_size}')\n",
    "\n",
    "# get the maximum length of a review\n",
    "max_review_length = int(df_preprocess.review_text.str.split().apply(len).max())\n",
    "print(f'Maximum review length: {max_review_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initial use a very simple `TextVectorization` layer with `'int'` output mode. In later iterations we can experiment with different vectorization layers,parameters, and output modes like `'tf-idf'` which is a simple, but quite effective technique in NLP text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(\n",
    "        texts:List[str],\n",
    "        max_tokens:int,\n",
    "        output_mode:str='int',\n",
    "        output_sequence_length:Optional[int]=None,\n",
    "        ngrams:Optional[int]=None):\n",
    "    \"\"\"Create and adapt a vectorization layer for the given texts.\"\"\"\n",
    "    # create the layer\n",
    "    text_vectorizer = layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=output_mode,\n",
    "        output_sequence_length=output_sequence_length,\n",
    "        ngrams=ngrams\n",
    "    )\n",
    "\n",
    "    # adapt the layer to the texts\n",
    "    text_vectorizer.adapt(texts)\n",
    "\n",
    "    return text_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorizer\n",
    "text_vectorizer_layer = create_vectorizer(\n",
    "    df_preprocess.review_text.values,\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_review_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the vectorizer\n",
    "sample_review = df_preprocess[df_preprocess.id == 3950575].review_text.values[0]\n",
    "display(Markdown(sample_review))\n",
    "print(f\"Vectorized review: {text_vectorizer_layer([sample_review])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Baseline\n",
    "\n",
    "For a multi-label text classification task in Natural Language Processing (NLP), a commonsense baseline could be designed using simple heuristics based on the frequency of specific keywords or phrases associated with each label. Multi-label classification differs from binary or multi-class classification in that each text instance can be associated with multiple labels simultaneously, rather than belonging to just one category.\n",
    "\n",
    "To accomplish this we could start with  simple libraries like the [Natural Laugage Toolkit (NLTK)](https://www.nltk.org/) to tokenize the text and count the frequency of specific words or phrases associated with each label. We could then use these frequencies to predict the labels for new text instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Label Powerset\n",
    "\n",
    "The \"Label Powerset\" method is an approach to multi-label classification where each unique combination of labels is treated as a single label in a multi-class classification problem. The method creates a new class for each combination of labels observed in the training set, effectively transforming the multi-label problem into a more traditional multi-class problem.\n",
    "\n",
    "In a prior section we determined the following combination as the most frequent occuring in `7.4%` of the dataset:\n",
    "\n",
    "`[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]`\n",
    "\n",
    "We can use this as a naive baseline to predict the most frequent label combination for all instances.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output tensor\n",
    "top_combination_tensor = tf.constant(top_combination, dtype=tf.float32)\n",
    "top_combination_tensor = tf.expand_dims(top_combination_tensor, 0)  # Shape becomes [1, num_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the naive model\n",
    "naive_model = keras.Sequential([\n",
    "    text_vectorizer_layer,\n",
    "    keras.layers.Lambda(lambda x: \n",
    "        tf.tile(top_combination_tensor, [tf.shape(x)[0], 1])\n",
    "    )\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "naive_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['binary_accuracy', 'accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(naive_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Create a text classification model\n",
    "\n",
    "In this section we create a very simple baseline classification model that tried to learn from the data. We will use a simple feed-forward neural network with a few dense layers and a sigmoid activation function to predict the presence of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline_model(\n",
    "        vectorization_layer: layers.TextVectorization,\n",
    "        output_units:int,\n",
    "        dense_units:(List[int]),\n",
    "        dense_activation:Optional[str]='relu') -> keras.Model:\n",
    "    \"\"\"Create a simple feed-forward model that can be used as a baseline.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add the vectorization layer\n",
    "    model.add(vectorization_layer)\n",
    "\n",
    "    # add the dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(layers.Dense(units, activation=dense_activation))\n",
    "\n",
    "    # add the output layer\n",
    "    model.add(layers.Dense(output_units, activation='sigmoid', name='output'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model() -> keras.Model:\n",
    "    \"\"\"Create the baseline model with specific hyperparameters\n",
    "    for reuse in other experiments.\n",
    "    \"\"\"\n",
    "    global text_vectorizer_layer\n",
    "    global lookup\n",
    "\n",
    "    return make_baseline_model(\n",
    "        text_vectorizer_layer,\n",
    "        output_units=len(lookup.get_vocabulary()),\n",
    "        dense_units=[512],\n",
    "        dense_activation='relu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the baseline model\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "baseline_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['binary_accuracy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = baseline_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Evaluate the model\n",
    "\n",
    "From the evaluation results below we can see that wile the baseline model appears to perform well when considering the **Categorical accuracy** it performs poorly when considering the **F1-Score** and **Jaccard Similarity**.\n",
    "\n",
    "First we need to understand the **Binary Accuracy** metric as reported by Keras. This metric treats each label as a separate binary classification problem. It calculates the percentage of correctly predicted labels (both positive and negative) across all labels and instances. High binary accuracy indicates that, on average, the model is good at deciding whether a specific label should be applied or not, but it doesn't consider the exact set of labels for each instance.\n",
    "\n",
    "##### Diagnosing the Issue\n",
    "\n",
    "1. **Class Imbalance**: If some labels are much more frequent than others, the model might learn to predict these labels more accurately, contributing to high binary accuracy. However, rare labels might be poorly predicted, which affects F1 scores and Jaccard similarity negatively.\n",
    "\n",
    "2. **Label Correlations**: In real-world data, labels can be correlated (e.g., \"Network Coverage\" and \"Service\" might often appear together). If the model fails to capture these correlations, it might still achieve good binary accuracy by getting many labels right based on their individual occurrences but fail at accurately capturing the relationships between labels, which is crucial for high F1 scores and Jaccard similarity.\n",
    "\n",
    "3. **Thresholding**: The way you convert model outputs (probabilities) into label predictions (binary) can significantly impact performance. The default threshold is 0.5, but adjusting this threshold might improve F1 scores and Jaccard similarity.\n",
    "\n",
    "##### Improving The Model\n",
    "\n",
    "The following are potential strategies for improving the model:\n",
    "\n",
    "1. **Model Architecture**: Experiment with different model architectures, layers, and hyperparameters. Attention mechanisms or more sophisticated RNNs and CNNs might capture label correlations better.\n",
    "\n",
    "2. **Class Weights**: Adjust class weights to handle imbalanced data better, making the model pay more attention to rare labels.\n",
    "\n",
    "3. **Evaluation Metric Optimization**: Instead of optimizing for binary accuracy during training, consider optimizing directly for metrics more aligned with your actual goals, like F1 score or Jaccard index, if possible.\n",
    "\n",
    "4. **Threshold Tuning**: Experiment with different thresholds for converting probabilities to binary labels, or consider using ranking-based metrics if thresholding proves problematic.\n",
    "\n",
    "\n",
    "> ❗️As discussed in a previous section, the dataset is imbalanced at both the label and label combination levels. This imbalance is likely contributing to the discrepancy between the binary accuracy and the F1 scores and Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, binary_acc, accuracy = baseline_model.evaluate(test_dataset)\n",
    "print(f\"Categorical accuracy on the test set: {round(binary_acc * 100, 2)}%.\")\n",
    "print(f\"Accuracy on the test set: {round(accuracy * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the relevant evaluation metrics using the `Evaluation` class for a more detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(baseline_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Class Weights\n",
    "\n",
    " The distribution of classes can significantly impact model performance. As already seen, our dataset exhibit a class imbalance, where some classes are underrepresented compared to others. This imbalance can lead to models that are biased towards the majority class, at the expense of accurately predicting minority class instances.\n",
    "\n",
    "To mitigate this issue, we will experiment with class weights. Class weighting is a technique that adjusts the importance of each class during the training process, based on the inverse of their frequencies. By assigning higher weights to underrepresented classes, the model is penalized more for misclassifying these classes, encouraging it to pay more attention to them.\n",
    "\n",
    "The method we will use involves calculating the class weights based on the training dataset's class distribution. These weights are then applied to the loss function during model training, effectively altering the training dynamics to favor an equitable representation of classes in the model's predictions. This approach is straightforward to implement and has been shown to improve model performance on imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Calculate Label Weights\n",
    "\n",
    "First, we need to determine the frequency of each label in your dataset. This involves counting how many times each label appears across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_weights(y_train):\n",
    "    \"\"\"Calculate label weights based on inverse label frequency.\"\"\"\n",
    "    label_frequencies = np.sum(y_train, axis=0)\n",
    "    label_weights = len(y_train) / (label_frequencies + 1e-9)  # Adding a small value to avoid division by zero\n",
    "    label_weights /= np.min(label_weights)  # Optional normalization\n",
    "    return label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train = np.concatenate([label_batch.numpy() for _, label_batch in train_dataset], axis=0)\n",
    "label_weights = calculate_label_weights(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Label Weights in Loss Calculation\n",
    "\n",
    "Define a custom loss metric based on `tf.keras.losses.binary_crossentropy` that incorporates the calculated class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Weighted binary cross-entropy loss for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: True labels.\n",
    "    - y_pred: Predictions.\n",
    "    - weights: Array of shape (num_labels,) containing the weight for each label.\n",
    "    \n",
    "    Returns:\n",
    "    - Weighted loss.\n",
    "    \"\"\"\n",
    "    # Convert weights to a tensor\n",
    "    weights = tf.constant(weights, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate the binary cross-entropy loss\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Expand weights to match the shape of y_true and y_pred\n",
    "    weights = tf.reduce_mean(weights * y_true, axis=-1)\n",
    "    \n",
    "    # Apply the weights\n",
    "    weighted_bce = bce * weights\n",
    "    \n",
    "    # Return the mean loss\n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "def get_weighted_loss(weights):\n",
    "    \"\"\"Implements wrapping the custom loss with the calculated weights\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        return weighted_binary_crossentropy(y_true, y_pred, weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the same baseline model architecture as before\n",
    "weight_model = create_baseline_model()\n",
    "\n",
    "# compile the model with the new metrics\n",
    "weight_model.compile(\n",
    "    loss=get_weighted_loss(label_weights),\n",
    "    optimizer='adam',\n",
    "    metrics=['binary_accuracy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = weight_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(weight_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the `binary_accuracy` metric is completely inappropriate for this multi-label classification task. Even when we attempt to apply class weights to the loss function, the `binary_accuracy` metric keeps improving, but the important metrics like `f1_score` and `jaccard_score` do not improve.\n",
    "\n",
    "An alternative metric list [Focal Loss](https://arxiv.org/abs/1708.02002) could potentially be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Focal Loss\n",
    "\n",
    "In the section we will experiment with using using Focal Loss as an alternative loss function. Additional training metrics are also introduced to monitor the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the same baseline model architecture as before\n",
    "focal_model = create_baseline_model()\n",
    "\n",
    "# compile the model with the new metrics\n",
    "focal_model.compile(\n",
    "    loss=keras_cv.losses.FocalLoss(alpha=0.5, gamma=2, label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = focal_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "plot_result(\"recall\")\n",
    "plot_result(\"precision\")\n",
    "plot_result(\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(focal_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2. Analysis\n",
    "\n",
    "Unfortunately the alternative loss function also did not improve the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Surrogate F1 Score Loss Function\n",
    "\n",
    "The root cause of the challenges we are facing is the loss function used for training does not align well with the performance metrics of interest (e.g., F1 Score, Jaccard Similarity, Subset Accuracy). `binary_crossentropy` effectively captures the probability error for each label independently, but it does not account for the interdependencies between labels or the balance among different classes which are crucial in a multi-label context. Consequently, the model may achieve a low value of binary_crossentropy indicating good performance from an optimization standpoint, yet it might score poorly on the more nuanced performance metrics that are critical for evaluating the practical effectiveness of the model.\n",
    "\n",
    "In this project the loss function should incorporate elements from the evaluation metrics that we care about (like F1 Score, Jaccard Similarity, or Hamming Loss). For instance, F1 Score considers both precision and recall, making it a more balanced metric for evaluating performance across diverse classes. However, since F1 Score is not differentiable and thus not directly usable in gradient-based optimization, we can create a differentiable surrogate that approximates it.\n",
    "\n",
    "n light of the challenges associated with aligning loss functions with evaluation metrics in multi-label classification tasks, our project will explore the implementation and effectiveness of a Smooth F1 Score based on the innovative approach presented in the [paper](https://arxiv.org/abs/2108.10566) _\"sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification\"_ by Gabriel Bénédict, Vincent Koops, Daan Odijk, and Maarten de Rijke. This paper introduces the sigmoidF1 loss function, a novel approximation of the F1 Score designed specifically for multi-label classification scenarios. Unlike traditional loss functions, sigmoidF1 is smooth and differentiable, making it suitable for optimization through stochastic gradient descent while directly targeting the nuances of multi-label metrics. The authors demonstrate the superior performance of the sigmoidF1 loss function across various datasets, including both text and image domains, showcasing its ability to more accurately estimate label propensities and counts compared to conventional approaches. By experimenting with the sigmoidF1 model, we aim to bridge the gap between optimization during training and the actual performance metrics that matter, potentially enhancing the model's practical effectiveness in handling complex multi-label classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1. sigmoidF1\n",
    "\n",
    "Code taken from: https://github.com/gabriben/metrics-as-losses/blob/main/VLAP/sigmoidF1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidF1(y, y_hat, from_logits = True):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        -\n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    S = -10 # sigmoid s-shape hyperparam (initial -10)\n",
    "    E = 1 # sigmoid offset hyperparam (initial 1)\n",
    "\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    \n",
    "    if from_logits == True:\n",
    "        # y = tf.nn.softmax(y)\n",
    "        #y_hat = tf.nn.softmax(y_hat)\n",
    "        y_hat = tf.math.sigmoid(y_hat) # (initial)\n",
    "        #y_hat = tf.math.exp(y_hat) / (tf.math.exp(y_hat) + 1)\n",
    "\n",
    "    #tf.print(y_hat)\n",
    "        \n",
    "    b = tf.constant(S, tf.float32)\n",
    "    c = tf.constant(E, tf.float32)\n",
    "    sig = 1 / (1 + tf.math.exp(b * (y_hat + c)))\n",
    "    #tf.print(sig)\n",
    "    tp = tf.reduce_sum(sig * y, axis=0)\n",
    "    fp = tf.reduce_sum(sig * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - sig) * y, axis=0)\n",
    "\n",
    "    sigmoid_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - sigmoid_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macroCost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macroCost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2. Model Training\n",
    "\n",
    "The initial experiment shows a highly volatile loss on the training data, this could be an indication of the following:\n",
    "\n",
    "1. The learning rate is too high (initial `0.001`)\n",
    "2. Batch size too small (initial `150`)\n",
    "3. Model Capacity: The model might be too complex (overfitting) or too simple (underfitting) for the given task (initial `[512]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the same baseline model architecture as before\n",
    "f1_model = make_baseline_model(\n",
    "        text_vectorizer_layer,\n",
    "        output_units=len(lookup.get_vocabulary()),\n",
    "        dense_units=[1024], # (initial [512])\n",
    "        dense_activation='relu'\n",
    "    )\n",
    "\n",
    "# compile the model with the new metrics\n",
    "f1_model.compile(\n",
    "    loss=sigmoidF1,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = f1_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "plot_result(\"recall\")\n",
    "plot_result(\"precision\")\n",
    "plot_result(\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(f1_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3. Analysis\n",
    "\n",
    "White the `sigmoidF1` appears to be more closely aligned with the training objective, it does not improve the model's performance, and in some aspects (Subset Accuracy) it performs worse than the previous models. In future work we well revisit this approach.\n",
    "\n",
    "This concludes experimentation attempting to handle the class imbalance either through class weights or alternative loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Embedding Layer\n",
    "\n",
    "For text classification, a common approach is to use an embedding layer followed by one or more dense layers. In this section we will extend our original baseline model by adding an embedding layer. The embedding layer learns a dense representation for each token in the input text. This dense representation is then used as the input to the following dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        vectorization_layer: layers.TextVectorization,\n",
    "        output_units:int,\n",
    "        dense_units:List[int]=None,\n",
    "        embedding_dim:int=None,\n",
    "        bidirectional_lstm:List[int]=None,\n",
    "        l2_reg:List[float]=None,\n",
    "        dropout_rate:List[float]=None,\n",
    "        dense_activation:Optional[str]='relu') -> keras.Model:\n",
    "    \"\"\"Create a simple feed-forward model that can be used as a baseline.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add the vectorization layer\n",
    "    model.add(vectorization_layer)\n",
    "\n",
    "    # add the embedding layer\n",
    "    if embedding_dim is not None:\n",
    "        model.add(layers.Embedding(\n",
    "            input_dim=len(vectorization_layer.get_vocabulary()),\n",
    "            output_dim=embedding_dim))\n",
    "\n",
    "    # add bidirectional LSTM layers\n",
    "    if bidirectional_lstm is not None:\n",
    "        for i, units in enumerate(bidirectional_lstm):\n",
    "            lstm_layer = layers.Bidirectional(\n",
    "                layers.LSTM(units, return_sequences=(i < len(bidirectional_lstm) - 1)))\n",
    "            \n",
    "            model.add(lstm_layer)\n",
    "            # add regularization if provided\n",
    "            if l2_reg is not None:\n",
    "                lstm_layer.kernel_regularizer=keras.regularizers.l2(l2_reg[i])\n",
    "\n",
    "    # add dense layers\n",
    "    if dense_units is not None:\n",
    "        # add a flatten label\n",
    "        model.add(layers.Flatten())      \n",
    "        #model.add(keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "        # add the dense layers\n",
    "        for i, units in enumerate(dense_units):\n",
    "            dense_layer = layers.Dense(units, activation=dense_activation)\n",
    "\n",
    "            # add regularization if provided\n",
    "            if l2_reg is not None:\n",
    "                dense_layer.kernel_regularizer=keras.regularizers.l2(l2_reg[i])\n",
    "\n",
    "            model.add(dense_layer)\n",
    "\n",
    "            # add the dropout layer\n",
    "            if dropout_rate is not None:\n",
    "                model.add(layers.Dropout(dropout_rate[i]))\n",
    "\n",
    "    # add the output layer\n",
    "    model.add(layers.Dense(output_units, activation='sigmoid', name='output'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1. Extra Layer Only\n",
    "\n",
    "We will use the same model architecture as the baseline model, but with the addition of an embedding layer. None of the other model parameters will be changed for this initial experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=text_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()), \n",
    "    embedding_dim=16, # (initial 16)\n",
    "    dense_units=[512],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "#plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimal Epochs\n",
    "\n",
    "Train a model for the optimal number of epochs and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after how many epochs does the model start to overfit\n",
    "val_loss = history.history['val_loss']\n",
    "optimal_epochs = np.argmin(val_loss)\n",
    "\n",
    "print(f'Overfitting at {optimal_epochs} epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=text_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    embedding_dim=16,\n",
    "    dense_units=[512],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=optimal_epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the optimal epochs results in decreased performance. This is another indication that we are not optomizing for the curret metric, and the \"correct\" approach as demonstrated in the Deep Learning for Python textbook or the Keras example for [multi-label classification](https://keras.io/examples/nlp/multi_label_classification/) cannot be taken as generic advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2. Weight Regularization\n",
    "\n",
    "From the previous plots we can see that the model is over fitting. To address this we will first introduce weight regularization to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=text_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    embedding_dim=16,\n",
    "    dense_units=[512],\n",
    "    l2_reg=[0.002],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3. Dropout Regularization\n",
    "\n",
    "In the previous section we can see that weight regularization combats over fitting up to a point, but when trained for more epochs we can still see this issue. To address this we will introduce dropout regularization to the model.\n",
    "\n",
    "At this point we also reduce the dense units from `512` to `128` to reduce the model complexity, as over fitting did not improve in the initial experiment with dropout.\n",
    "\n",
    ">❗️It is important to note though that we are actually now getting worse performing models as measured by the nuanced metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=text_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    embedding_dim=16,\n",
    "    dense_units=[128],\n",
    "    l2_reg=[0.002],\n",
    "    dropout_rate=[0.5],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4. Regularization with sigmoidF1\n",
    "\n",
    "We will now experiment with the custom `sigmoidF1` loss function in combination with the dropout regularization.\n",
    "\n",
    "1. From the experiment below we can see that the `sigmoidF1` performs better now that we have reduced the model capacity and introduced regularization.\n",
    "\n",
    "2. It is also worth noting that we now achive the highest `F1 Score (Weighted-Average)` of all the models. However, this is at the expense of the `Subset Accuracy` which has now dropped to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=text_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    embedding_dim=16,\n",
    "    dense_units=[128],\n",
    "    l2_reg=[0.002],\n",
    "    dropout_rate=[0.5],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=sigmoidF1,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. TF-IDF Vectorization\n",
    "\n",
    "In this section we will experiment with using the `tf-idf` vectorization method instead of using an embedding layer. This is a simple, but quite effective technique in NLP text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorization layer\n",
    "tf_vectorizer_layer = create_vectorizer(\n",
    "    df_preprocess.review_text.values,\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='tf-idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=tf_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    dense_units=[128],\n",
    "    l2_reg=[0.002],\n",
    "    dropout_rate=[0.5],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.2. Analysis\n",
    "\n",
    "It is significant to note that by using the `tf-idf` we are able to outperform the Regularized embedding model with the binary crossentropy loss function.\n",
    "\n",
    "> This could indicate that our training set is too small to effectively learn the token embeddings.\n",
    "\n",
    "Specifically in the **Validation** and **Test** sets we now see the highes `F1 Score (Weighted-Average)` of all the models. However this is not reflected in the training set where we would have expected the highest scores, once again probably a reflection of the class imbalance. \n",
    "\n",
    "> In later sections it might be useful to experiment with **k-fold cross validation** to get a better understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Bidirectional LSTM\n",
    "\n",
    "In this experiment we will use a Bidirectional LSTM model to see if we can improve the model's performance. Bidirectional LSTMs are a type of recurrent neural network that can capture dependencies in both directions of a sequence, making them well-suited for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "#device.reset()\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1 LSTM Only\n",
    "\n",
    "Create a model with no dense layers, only a single LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = make_model(\n",
    "    vectorization_layer=tf_vectorizer_layer,\n",
    "    output_units=len(lookup.get_vocabulary()),\n",
    "    embedding_dim=16,\n",
    "    bidirectional_lstm=[16],\n",
    "    l2_reg=[0.002],\n",
    "    dense_activation='relu'\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=15,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2. Analysis\n",
    "\n",
    "❗️Although this model still produced comparable accuracy and binary_accuracy results, it completely fails as measured by the nuanced metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tansformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mixed precision to speed up training\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_preset = 'bert_base_en_uncased'\n",
    "bert_preset = 'bert_tiny_en_uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessor and backbone\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    bert_preset, sequence_length=512)\n",
    "backbone = keras_nlp.models.BertBackbone.from_preset(\n",
    "    bert_preset)\n",
    "\n",
    "# the backbone should not be trainable\n",
    "backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input layer\n",
    "inputs = keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessing = preprocessor(inputs)\n",
    "sequence = backbone(preprocessing)[\"sequence_output\"]\n",
    "\n",
    "# add the trainable layers\n",
    "for _ in range(3):\n",
    "    # sequence = keras.layers.Dense(128, activation='relu')(sequence)\n",
    "    # sequence = keras.layers.Dropout(0.2)(sequence)\n",
    "\n",
    "    sequence = keras_nlp.layers.TransformerEncoder(\n",
    "        num_heads=2,\n",
    "        intermediate_dim=512,\n",
    "        dropout=0.1,\n",
    "    )(sequence)\n",
    "\n",
    "# Flatten the output of the transformer layer\n",
    "#flattened_sequence = keras.layers.Flatten()(sequence)\n",
    "flattened_sequence = keras.layers.GlobalMaxPooling1D()(sequence)\n",
    "\n",
    "# create the output layer\n",
    "output_layer = keras.layers.Dense(\n",
    "    len(lookup.get_vocabulary()), \n",
    "    activation='sigmoid')(flattened_sequence)\n",
    "\n",
    "# create the model\n",
    "model = keras.Model(inputs, output_layer)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0, axis=0),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5), # initial 5e-5\n",
    "    metrics=['accuracy','binary_accuracy'],\n",
    ")\n",
    "\n",
    "# show the model summary\n",
    "#print(model.summary())\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs, # 158 found for the highest val_accuracy\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")\n",
    "\n",
    "# evaluate the model\n",
    "print(model.evaluate(test_dataset))\n",
    "\n",
    "# show the evaluation\n",
    "show_model_evaluation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features from the test set\n",
    "test_features = np.concatenate([feature_batch.numpy() for feature_batch, _ in test_dataset], axis=0)\n",
    "\n",
    "# get the actual and predicted values\n",
    "y_true = np.concatenate([label_batch.numpy() for _, label_batch in test_dataset], axis=0)\n",
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "# lookup the label values\n",
    "y_true_labels = [invert_multi_hot(label, lookup) for label in y_true]\n",
    "y_pred_labels = [invert_multi_hot(label, lookup) for label in y_pred > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display n random samples\n",
    "n = 5\n",
    "random_indices = np.random.choice(len(test_features), n, replace=False)\n",
    "\n",
    "for i in random_indices:\n",
    "    print(f\"Review: {test_features[i]}\")\n",
    "    print(f\"True labels:      {y_true_labels[i]}\")\n",
    "    print(f\"Predicted labels: {y_pred_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the actual labels in the test set\n",
    "all_true_labels = [label for labels in y_true_labels for label in labels]\n",
    "pd.DataFrame(all_true_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the predicted labels in the test set\n",
    "all_pred_labels = [label for labels in y_pred_labels for label in labels]\n",
    "pd.DataFrame(all_pred_labels).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-intent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
