{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-02 : Multi-label text classification\n",
    "\n",
    "After extracting intents, we use Keras, a comprehensive deep learning library, to develop a multi-class classification model.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Large-scale multi-label text classification](https://keras.io/examples/nlp/multi_label_classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:57:49.452910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 19:57:49.452938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 19:57:49.453826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-17 19:57:49.458011: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 19:57:49.962439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "orig_data_path = f'{data_path}/hellopeter'\n",
    "orig_file = f'{orig_data_path}/00-01_vodacom_selected_reviews.parquet.gz'\n",
    "\n",
    "intent_path = f'{data_path}/multiclass_model'\n",
    "intent_extract_file = f'{intent_path}/01-03_intents.parquet.gz'\n",
    "intent_file = f'{intent_path}/02-01_flat_intents.parquet.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Lineage\n",
    "\n",
    "1. The original dataset is a collection publicly accessible customer reviews/complaints scraped from [Hellopeter](https://www.hellopeter.com/) site between 2021 and 2023. This dataset was [created](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/01_hellopeter/01-01_retrieve_data.ipynb) in another research project investigating: [Evaluating Customer Satisfaction and Preferences in the Telecommunications Industry: A Comparative Analysis of Survey Data and Online Reviews](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/04_draft/04-04_cw02.ipynb)\n",
    "\n",
    "2. Data selection was perform in the `00-01_data_selection` notebook based on the following criteria.\n",
    "    - Reviews from the for the 5 month period from **2022-06-01** to **2023-06-30** were selected.\n",
    "\n",
    "    - Only reviews from the **Vodacom** telecommunications company were selected.\n",
    "\n",
    "    - Very short, or very long reviews were removed. Reviews between from **10** to **100** words were selected. The word count was calculated using a basic `.str.split().str.len()` which is sufficient for this purpose.\n",
    "\n",
    "3. The unlabeled data was then labeled in the `01-02_batch_classification` notebook using Generative AI.\n",
    "    - The **Mistral 7B v0.2** Large Language Model (LLM) were hosted on the local servier with [Ollama](https://ollama.com/library/mistral). Please refer to the `01-01_classification_test` notebook for further details.\n",
    "\n",
    "    - The classification was done using multiple prompts similar to Chain-of-Thought (CoT) techniques for classification. _Implementation details can be found in the `src` directory._\n",
    "\n",
    "    - Classification was done based on the categories defined in `src/config/category_definitions.jsonl`.\n",
    "\n",
    "    - It took an average of **7 seconds** to classify a single review.\n",
    "\n",
    "4. Using Generative AI for labeling introduced new categories that were cleaned up in the `01-03_cleanup` notebook.\n",
    "\n",
    "    - First new categories that were prefixed with an original category were replaced.\n",
    "\n",
    "    - Then, new categories that contained an original category in round brackets were replaced with the original.\n",
    "\n",
    "    - For the remaining new categories, the reviews were manually inspected and the reviews were reclassified via manual mapping.\n",
    "\n",
    "5. The data labels was then converted into a format suitable for modeling in the `02-01_data_preperation` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Data\n",
    "\n",
    "The following shows a sample customer review from the original source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>3950575</td>\n",
       "      <td>Vodacom is useless!!!</td>\n",
       "      <td>Good day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           review_title  \\\n",
       "5215  3950575  Vodacom is useless!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   review_content  \n",
       "5215  Good day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_source = pd.read_parquet(orig_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_source[df_source.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Labels\n",
    "\n",
    "A sample of the labels extracted using the LLM is shown below. From this we can see that the LLM has extracted multiple labels for each review, and a reason is generated form each label for human verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>reason</th>\n",
       "      <th>relevance</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Billing</td>\n",
       "      <td>The text mentions that an amount was debited from the account twice.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Cancellation</td>\n",
       "      <td>The text expresses the intent to cancel all contracts with Vodacom due to the billing issue.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Customer's Feeling</td>\n",
       "      <td>The text contains a negative sentiment towards Vodacom.</td>\n",
       "      <td>0.5</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category  \\\n",
       "29             Billing   \n",
       "30        Cancellation   \n",
       "31  Customer's Feeling   \n",
       "\n",
       "                                                                                          reason  \\\n",
       "29                          The text mentions that an amount was debited from the account twice.   \n",
       "30  The text expresses the intent to cancel all contracts with Vodacom due to the billing issue.   \n",
       "31                                       The text contains a negative sentiment towards Vodacom.   \n",
       "\n",
       "    relevance sentiment       id  \n",
       "29        1.0  negative  3950575  \n",
       "30        1.0  negative  3950575  \n",
       "31        0.5  negative  3950575  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_intents_extracted = pd.read_parquet(intent_extract_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents_extracted[df_intents_extracted.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared Data Labels\n",
    "\n",
    "The prepared data labels are shown below. The data labels are prepared for multi-label classification.\n",
    "\n",
    "This data will need to be combined with the `review_title` and `review_content` from the original source data to create the final dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category_list</th>\n",
       "      <th>relevance_list</th>\n",
       "      <th>sentiment_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3950575</td>\n",
       "      <td>[Billing, Cancellation, Customer's Feeling]</td>\n",
       "      <td>[1.0, 1.0, 0.5]</td>\n",
       "      <td>[negative, negative, negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                category_list   relevance_list  \\\n",
       "2  3950575  [Billing, Cancellation, Customer's Feeling]  [1.0, 1.0, 0.5]   \n",
       "\n",
       "                   sentiment_list  \n",
       "2  [negative, negative, negative]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples: 5218\n"
     ]
    }
   ],
   "source": [
    "df_intents = pd.read_parquet(intent_file)\n",
    "df_intents[\"category_list\"] = df_intents[\"category_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"relevance_list\"] = df_intents[\"relevance_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"sentiment_list\"] = df_intents[\"sentiment_list\"].apply(lambda x: list(x))\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents[df_intents.id == 3950575]\n",
    "    )\n",
    "\n",
    "print(f'Data samples: {len(df_source)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', \"Customer's Feeling\", 'Billing', 'Network Coverage', 'Cancellation', 'Call Center', 'Policy', 'Account Management', 'Response', 'Resolution', 'Devices', 'Staff Level', 'Price Plans', 'Brand', 'Abuse', 'Products', 'Service', 'Services', 'SIM', 'Other']\n",
      "Vocabulary size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:57:50.601545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.631545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.631881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.633215: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.636004: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.636381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.693545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.693719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.693875: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 19:57:50.693991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9376 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# get the list of category target values\n",
    "categories = tf.ragged.constant(df_intents.category_list.values)\n",
    "\n",
    "# learn the vocabulary\n",
    "lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(categories)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(lookup.get_vocabulary())\n",
    "print(f'Vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sample illustrates the binarization of the multi-labels. The multiple labels are encoded into a binary matrix where the positions corresponding with the labels have a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['Billing', 'Cancellation', \"Customer's Feeling\"]\n",
      "Label-binarized representation: [[0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# find a sample label to test the lookup\n",
    "sample_label = df_intents[df_intents.id == 3950575].category_list.values[0]\n",
    "\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "# binarize the label\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `invert_multi_hot` function is used to convert the binary matrix back into the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Customer's Feeling\", 'Billing', 'Cancellation'], dtype='<U18')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_labels, lookup:keras.layers.StringLookup):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    # get the vocabulary\n",
    "    vocab = lookup.get_vocabulary()\n",
    "\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "## test the inverse function\n",
    "invert_multi_hot(label_binarized[0], lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare Dataset\n",
    "\n",
    "Prepare a dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3950516</td>\n",
       "      <td>**Vodacom fraudster**\\n\\nVodacom is a scam! Never ever take, a contract  with those people. I had a, contract ending end October. End August I called them and cancelled the contract. I was suprised to find myself at credit bureau while I was looking for a house bond. They didn't cancel my contract. I call them, the system shows I indeed cancel the contract but they don't know why t wasn't cancelled. They are taking me from pillar to post and my life is at a, standstill. Fraudsters</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3950535</td>\n",
       "      <td>**bad service**\\n\\nstill awating any feedback from vodacom legal department ant the email address  of DCA Hammond Pole, so that I can forward him all the mails to vodacom that has not been responded by Vodacom, and as stated two times allready, I dont have my number any more so cant phone the DCA, the messages has also been ignored by Vodacom</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3950575</td>\n",
       "      <td>**Vodacom is useless!!!**\\n\\nGood day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  3950516   \n",
       "1  3950535   \n",
       "2  3950575   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             review_text  \\\n",
       "0  **Vodacom fraudster**\\n\\nVodacom is a scam! Never ever take, a contract  with those people. I had a, contract ending end October. End August I called them and cancelled the contract. I was suprised to find myself at credit bureau while I was looking for a house bond. They didn't cancel my contract. I call them, the system shows I indeed cancel the contract but they don't know why t wasn't cancelled. They are taking me from pillar to post and my life is at a, standstill. Fraudsters   \n",
       "1                                                                                                                                               **bad service**\\n\\nstill awating any feedback from vodacom legal department ant the email address  of DCA Hammond Pole, so that I can forward him all the mails to vodacom that has not been responded by Vodacom, and as stated two times allready, I dont have my number any more so cant phone the DCA, the messages has also been ignored by Vodacom   \n",
       "2                                                                                                                                                                               **Vodacom is useless!!!**\\n\\nGood day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.   \n",
       "\n",
       "                                                                                       category_encoded  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start from the original label dataset\n",
    "df_preprocess = df_intents.copy()\n",
    "\n",
    "# binarize the category_list\n",
    "labels = tf.ragged.constant(df_preprocess[\"category_list\"].values)\n",
    "label_binarized = lookup(labels).numpy()\n",
    "df_preprocess[\"category_encoded\"] = label_binarized.tolist()\n",
    "\n",
    "# add the review content\n",
    "df_preprocess = df_preprocess \\\n",
    "    .set_index('id') \\\n",
    "    .join(df_source.set_index('id'), how='left') \\\n",
    "    .assign(review_text=lambda x: '**' + x.review_title + '**\\n\\n' + x.review_content) \\\n",
    "    .reset_index()\n",
    "\n",
    "# select the relevant columns\n",
    "df_preprocess = df_preprocess[['id', 'review_text', 'category_encoded']]\n",
    "\n",
    "# show a sample of the preprocessed data\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_preprocess.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3. Train Test Split\n",
    "\n",
    "In a multi-label classification problem, imbalance can occur at two levels:\n",
    "\n",
    "1. **Label imbalance**: Some labels appear more frequently than others.\n",
    "2. **Label combination imbalance**: Some combinations of labels appear more frequently than others.\n",
    "\n",
    "Both these imbalances are present in the dataset. Imbalance can lead to a model that performs well on the majority classes but poorly on the minority classes. This is because the model might be biased towards predicting the majority classes due to their higher occurrence in the training data.\n",
    "\n",
    "To address this, we will ideally use a stratified split to ensure that the distribution of labels in the training and validation sets is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination Category Count  : 376\n",
      "Combinations with one sample: 128\n"
     ]
    }
   ],
   "source": [
    "df_category_count = df_intents.category_list.value_counts().reset_index()\n",
    "df_category_count.columns = ['category', 'samples']\n",
    "\n",
    "print(f'Combination Category Count  : {len(df_category_count)}')\n",
    "print(f'Combinations with one sample: {len(df_category_count[df_category_count.samples == 1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we can see that about a third (0.34) of the unique category combinations have only one sample. This means that we will not be able to use a stratified split, as the validation set will not contain any of these unique combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set   :  3214\n",
      "Number of rows in validation set :   803\n",
      "Number of rows in test set       :  1005\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# initial train and test split\n",
    "train_full_df, test_df = train_test_split(\n",
    "    df_intents,\n",
    "    test_size=test_split\n",
    ")\n",
    "\n",
    "# splitting the train set further into validation and new train sets\n",
    "val_df = train_full_df.sample(frac=0.2)\n",
    "train_df = train_full_df.drop(val_df.index)\n",
    "\n",
    "# select only the id column\n",
    "train_full_ids = train_full_df.id.values\n",
    "train_ids = train_df.id.values\n",
    "val_ids = val_df.id.values\n",
    "test_ids = test_df.id.values\n",
    "\n",
    "# show the record counts per dataset\n",
    "print(f\"Number of rows in training set   : {len(train_ids):>5}\")\n",
    "print(f\"Number of rows in validation set : {len(val_ids):>5}\")\n",
    "print(f\"Number of rows in test set       : {len(test_ids):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Vectorization\n",
    "\n",
    "Before we can train a model, we need to convert the text data into a format that can be fed into the model. There are multiple ways to do this, but a common approach is to convert text data into numerical data using a method such as TF-IDF, word embeddings, or a pre-trained language model.\n",
    "\n",
    "For this purpose we will use the a `TextVectorization` layer that can operate as part of the main model so that the model is excluded from the core preprocessing logic. As noted in the [Keras documentation](https://keras.io/examples/nlp/multi_label_classification/) this also greatly reduces the chances of training / serving skew during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of unique words in the dataset and the maximum length of a review.  \n",
    "_We expect the maximum review length to be 100 words as we have filtered the reviews to be between 10 and 100 words._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20646\n",
      "Maximum review length: 100\n"
     ]
    }
   ],
   "source": [
    "# calculate the number of unique words in the dataset\n",
    "vocabulary = set()\n",
    "df_preprocess.review_text.str.lower().str.split().apply(vocabulary.update)\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "\n",
    "# get the maximum length of a review\n",
    "max_review_length = df_preprocess.review_text.str.split().apply(len).max()\n",
    "print(f'Maximum review length: {max_review_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initial use a very simple `TextVectorization` layer with `'int'` output mode. In later iterations we can experiment with different vectorization layers,parameters, and output modes like `'tf-idf'` which is a simple, but quite effective technique in NLP text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline\n",
    "\n",
    "For a multi-label text classification task in Natural Language Processing (NLP), a commonsense baseline could be designed using simple heuristics based on the frequency of specific keywords or phrases associated with each label. Multi-label classification differs from binary or multi-class classification in that each text instance can be associated with multiple labels simultaneously, rather than belonging to just one category.\n",
    "\n",
    "To accomplish this we could start with  simple libraries like the [Natural Laugage Toolkit (NLTK)](https://www.nltk.org/) to tokenize the text and count the frequency of specific words or phrases associated with each label. We could then use these frequencies to predict the labels for new text instances. However for the sake of simplicity we will instead start with a very basic deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-intent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
