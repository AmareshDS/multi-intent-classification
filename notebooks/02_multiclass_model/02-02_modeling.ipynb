{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-02 : Multi-label text classification\n",
    "\n",
    "After extracting intents, we use Keras, a comprehensive deep learning library, to develop a multi-class classification model.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Large-scale multi-label text classification](https://keras.io/examples/nlp/multi_label_classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:06:21.989138: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 18:06:21.989165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 18:06:21.990032: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-17 18:06:21.994129: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 18:06:22.494780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "orig_data_path = f'{data_path}/hellopeter'\n",
    "orig_file = f'{orig_data_path}/00-01_vodacom_selected_reviews.parquet.gz'\n",
    "\n",
    "intent_path = f'{data_path}/multiclass_model'\n",
    "intent_extract_file = f'{intent_path}/01-03_intents.parquet.gz'\n",
    "intent_file = f'{intent_path}/02-01_flat_intents.parquet.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Lineage\n",
    "\n",
    "1. The original dataset is a collection publicly accessible customer reviews/complaints scraped from [Hellopeter](https://www.hellopeter.com/) site between 2021 and 2023. This dataset was [created](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/01_hellopeter/01-01_retrieve_data.ipynb) in another research project investigating: [Evaluating Customer Satisfaction and Preferences in the Telecommunications Industry: A Comparative Analysis of Survey Data and Online Reviews](https://github.com/JohnnyFoulds/dsm050-2023-apr/blob/master/notebooks/04_draft/04-04_cw02.ipynb)\n",
    "\n",
    "2. Data selection was perform in the `00-01_data_selection` notebook based on the following criteria.\n",
    "    - Reviews from the for the 5 month period from **2022-06-01** to **2023-06-30** were selected.\n",
    "\n",
    "    - Only reviews from the **Vodacom** telecommunications company were selected.\n",
    "\n",
    "    - Very short, or very long reviews were removed. Reviews between from **10** to **100** words were selected. The word count was calculated using a basic `.str.split().str.len()` which is sufficient for this purpose.\n",
    "\n",
    "3. The unlabeled data was then labeled in the `01-02_batch_classification` notebook using Generative AI.\n",
    "    - The **Mistral 7B v0.2** Large Language Model (LLM) were hosted on the local servier with [Ollama](https://ollama.com/library/mistral). Please refer to the `01-01_classification_test` notebook for further details.\n",
    "\n",
    "    - The classification was done using multiple prompts similar to Chain-of-Thought (CoT) techniques for classification. _Implementation details can be found in the `src` directory._\n",
    "\n",
    "    - Classification was done based on the categories defined in `src/config/category_definitions.jsonl`.\n",
    "\n",
    "    - It took an average of **7 seconds** to classify a single review.\n",
    "\n",
    "4. Using Generative AI for labeling introduced new categories that were cleaned up in the `01-03_cleanup` notebook.\n",
    "\n",
    "    - First new categories that were prefixed with an original category were replaced.\n",
    "\n",
    "    - Then, new categories that contained an original category in round brackets were replaced with the original.\n",
    "\n",
    "    - For the remaining new categories, the reviews were manually inspected and the reviews were reclassified via manual mapping.\n",
    "\n",
    "5. The data labels was then converted into a format suitable for modeling in the `02-01_data_preperation` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Data\n",
    "\n",
    "The following shows a sample customer review from the original source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>3950575</td>\n",
       "      <td>Vodacom is useless!!!</td>\n",
       "      <td>Good day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           review_title  \\\n",
       "5215  3950575  Vodacom is useless!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   review_content  \n",
       "5215  Good day\\n\\nAgain, vodacom did not do their jobs. The amount went off as I explicitly asked for it not to. Vodacom now owes me R300 as it has been debited from my account twice now. I will be taking this to social media now. And I want to please cancel all my contracts with vodacom.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_source = pd.read_parquet(orig_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_source[df_source.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Labels\n",
    "\n",
    "A sample of the labels extracted using the LLM is shown below. From this we can see that the LLM has extracted multiple labels for each review, and a reason is generated form each label for human verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>reason</th>\n",
       "      <th>relevance</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Billing</td>\n",
       "      <td>The text mentions that an amount was debited from the account twice.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Cancellation</td>\n",
       "      <td>The text expresses the intent to cancel all contracts with Vodacom due to the billing issue.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Customer's Feeling</td>\n",
       "      <td>The text contains a negative sentiment towards Vodacom.</td>\n",
       "      <td>0.5</td>\n",
       "      <td>negative</td>\n",
       "      <td>3950575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category  \\\n",
       "29             Billing   \n",
       "30        Cancellation   \n",
       "31  Customer's Feeling   \n",
       "\n",
       "                                                                                          reason  \\\n",
       "29                          The text mentions that an amount was debited from the account twice.   \n",
       "30  The text expresses the intent to cancel all contracts with Vodacom due to the billing issue.   \n",
       "31                                       The text contains a negative sentiment towards Vodacom.   \n",
       "\n",
       "    relevance sentiment       id  \n",
       "29        1.0  negative  3950575  \n",
       "30        1.0  negative  3950575  \n",
       "31        0.5  negative  3950575  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_intents_extracted = pd.read_parquet(intent_extract_file)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents_extracted[df_intents_extracted.id == 3950575]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared Data Labels\n",
    "\n",
    "The prepared data labels are shown below. The data labels are prepared for multi-label classification.\n",
    "\n",
    "This data will need to be combined with the `review_title` and `review_content` from the original source data to create the final dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category_list</th>\n",
       "      <th>relevance_list</th>\n",
       "      <th>sentiment_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3950575</td>\n",
       "      <td>[Billing, Cancellation, Customer's Feeling]</td>\n",
       "      <td>[1.0, 1.0, 0.5]</td>\n",
       "      <td>[negative, negative, negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                category_list   relevance_list  \\\n",
       "2  3950575  [Billing, Cancellation, Customer's Feeling]  [1.0, 1.0, 0.5]   \n",
       "\n",
       "                   sentiment_list  \n",
       "2  [negative, negative, negative]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples: 5218\n"
     ]
    }
   ],
   "source": [
    "df_intents = pd.read_parquet(intent_file)\n",
    "df_intents[\"category_list\"] = df_intents[\"category_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"relevance_list\"] = df_intents[\"relevance_list\"].apply(lambda x: list(x))\n",
    "df_intents[\"sentiment_list\"] = df_intents[\"sentiment_list\"].apply(lambda x: list(x))\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(\n",
    "        df_intents[df_intents.id == 3950575]\n",
    "    )\n",
    "\n",
    "print(f'Data samples: {len(df_source)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', \"Customer's Feeling\", 'Billing', 'Network Coverage', 'Cancellation', 'Call Center', 'Policy', 'Account Management', 'Response', 'Resolution', 'Devices', 'Staff Level', 'Price Plans', 'Brand', 'Abuse', 'Products', 'Service', 'Services', 'SIM', 'Other']\n",
      "Vocabulary size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:06:23.136442: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.164127: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.164325: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.165424: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.165582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.165726: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.220910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.221082: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.221235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 18:06:23.221350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9376 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# get the list of category target values\n",
    "categories = tf.ragged.constant(df_intents.category_list.values)\n",
    "\n",
    "# learn the vocabulary\n",
    "lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(categories)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(lookup.get_vocabulary())\n",
    "print(f'Vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Test Split\n",
    "\n",
    "In a multi-label classification problem, imbalance can occur at two levels:\n",
    "\n",
    "1. **Label imbalance**: Some labels appear more frequently than others.\n",
    "2. **Label combination imbalance**: Some combinations of labels appear more frequently than others.\n",
    "\n",
    "Both these imbalances are present in the dataset. Imbalance can lead to a model that performs well on the majority classes but poorly on the minority classes. This is because the model might be biased towards predicting the majority classes due to their higher occurrence in the training data.\n",
    "\n",
    "To address this, we will ideally use a stratified split to ensure that the distribution of labels in the training and validation sets is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination Category Count  : 376\n",
      "Combinations with one sample: 128\n"
     ]
    }
   ],
   "source": [
    "df_category_count = df_intents.category_list.value_counts().reset_index()\n",
    "df_category_count.columns = ['category', 'samples']\n",
    "\n",
    "print(f'Combination Category Count  : {len(df_category_count)}')\n",
    "print(f'Combinations with one sample: {len(df_category_count[df_category_count.samples == 1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we can see that about a third (0.34) of the unique category combinations have only one sample. This means that we will not be able to use a stratified split, as the validation set will not contain any of these unique combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set   :  3214\n",
      "Number of rows in validation set :   803\n",
      "Number of rows in test set       :  1005\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.2\n",
    "\n",
    "# initial train and test split\n",
    "train_full_df, test_df = train_test_split(\n",
    "    df_intents,\n",
    "    test_size=test_split\n",
    ")\n",
    "\n",
    "# splitting the train set further into validation and new train sets\n",
    "val_df = train_full_df.sample(frac=0.2)\n",
    "train_df = train_full_df.drop(val_df.index)\n",
    "\n",
    "print(f\"Number of rows in training set   : {len(train_df):>5}\")\n",
    "print(f\"Number of rows in validation set : {len(val_df):>5}\")\n",
    "print(f\"Number of rows in test set       : {len(test_df):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline\n",
    "\n",
    "For a multi-label text classification task in Natural Language Processing (NLP), a commonsense baseline could be designed using simple heuristics based on the frequency of specific keywords or phrases associated with each label. Multi-label classification differs from binary or multi-class classification in that each text instance can be associated with multiple labels simultaneously, rather than belonging to just one category.\n",
    "\n",
    "To accomplish this we could start with  simple libraries like the [Natural Laugage Toolkit (NLTK)](https://www.nltk.org/) to tokenize the text and count the frequency of specific words or phrases associated with each label. We could then use these frequencies to predict the labels for new text instances. However for the sake of simplicity we will instead start with a very basic deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-intent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
